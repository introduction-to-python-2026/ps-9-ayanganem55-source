# -*- coding: utf-8 -*-
"""parkinsons.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/yotam-biu/ps9/blob/main/parkinsons.ipynb
"""

# Download the data from your GitHub repository
!wget https://raw.githubusercontent.com/yotam-biu/ps9/main/parkinsons.csv -O /content/parkinsons.csv
!wget https://raw.githubusercontent.com/yotam-biu/python_utils/main/lab_setup_do_not_edit.py -O /content/lab_setup_do_not_edit.py
import lab_setup_do_not_edit

"""

## 1. **Load the dataset:**  

   After running the first cell of this notebook, the file `parkinson.csv` will appear in the `Files` folder.
   You need to loaded the file as a DataFrame.  


"""

import pandas as pd
df = pd.read_csv('/content/parkinsons.csv')
display(df.head())

"""## 2. **Select features:**  

   - Choose **two features** as inputs for the model.  
   - Identify **one feature** to use as the output for the model.  

  #### Advice:  
  - You can refer to the paper available in the GitHub repository for insights into the dataset and guidance on identifying key features for the input and output.  
  - Alternatively, consider creating pair plots or using other EDA methods we learned in the last lecture to explore the relationships between features and determine which ones are most relevant.  

"""

X = df[['MDVP:Fo(Hz)', 'MDVP:Jitter(%)']]
y = df['status']

print("Input features (X) selected:")
display(X.head())
print("Output feature (y) selected:")
display(y.head())

"""## 3. **Scale the data:**

   Apply the `MinMaxScaler` to scale the two input columns to a range between 0 and 1.  

"""

from sklearn.preprocessing import MinMaxScaler

X_scaler = MinMaxScaler()
X_scaled = X_scaler.fit_transform(X)

print("Scaled Input features (X_scaled) selected:")
display(pd.DataFrame(X_scaled, columns=X.columns).head())

"""## 4. **Split the data:**

   Divide the dataset into a training set and a validation set.




"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

"""## 5. **Choose a model:**  

   Select a model to train on the data.  

   #### Advice:  
   - Consider using the model discussed in the paper from the GitHub repository as a reference.  

"""

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(random_state=42)

print("Model selected:")
display(model)

"""# 6. **Test the accuracy:**  

   Evaluate the model's accuracy on the test set. Ensure that the accuracy is at least **0.8**.  

"""

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

if accuracy >= 0.8:
    print("Accuracy is sufficient (>= 0.8).")
else:
    print("Accuracy is below 0.8. Consider further tuning or a different model.")

"""## 7. **Save and upload the model:**  

   After you are happy with your results, save the model with the `.joblib` extension and upload it to your GitHub repository main folder.
   
   Additionally, update the `config.yaml` file with the list of selected features and the model's joblib file name.  


example:  
```yaml
selected_features: ["A", "B"]  
path: "my_model.joblib"  
```
"""

import joblib

joblib.dump(model, 'my_model.joblib')

"""## 8. **Copy the code:**  

   Copy and paste all the code from this notebook into a `main.py` file in the GitHub repository.  

"""
